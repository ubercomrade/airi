{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465a62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import (\n",
    "    AUROC,\n",
    ")\n",
    "from torchmetrics import (\n",
    "    PearsonCorrCoef,\n",
    "    SpearmanCorrCoef,\n",
    "    R2Score\n",
    ")\n",
    "\n",
    "import ast\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748da9f",
   "metadata": {},
   "source": [
    "# Check data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67c6031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/user11/data/data_processed/data.tsv\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "embeddings_table = pd.read_csv(\"/home/user11/data/embeddings_proteins/wide_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "i = 1\n",
    "\n",
    "train = pd.read_csv(f\"/home/user11/data/data_processed/train{i}\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "train.hla = train.hla.str.replace(\"_\", \"\")\n",
    "train_data = pd.merge(train, embeddings_table, on=[\"peptide\", \"score\", \"hla\"])\n",
    "\n",
    "val = pd.read_csv(f\"/home/user11/data/data_processed/test{i}\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "val.hla = val.hla.str.replace(\"_\", \"\")\n",
    "val_data = pd.merge(val, embeddings_table, on=[\"peptide\", \"score\", \"hla\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28ec3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peptide                                                DLDKKETVWHLEE\n",
       "score                                                            0.0\n",
       "hla                                            HLA-DPA10103-DPB10201\n",
       "alpha_id                                                    DPA10103\n",
       "beta_id                                                     DPB10201\n",
       "alpha_seq          MRPEDRMFHIRAVILRALSLAFLLSLRGAGAIKADHVSTYAAFVQT...\n",
       "beta_seq           MMVLQVSAAPRTVALTALLMVLLTSVVQGRATPENYLFQGRQECYA...\n",
       "alpha_path         /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "beta_path          /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "interface                         YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT\n",
       "peptide_path       /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "alpha_positions    [39, 41, 52, 54, 61, 82, 83, 88, 89, 91, 95, 9...\n",
       "beta_positions     [37, 39, 38, 52, 54, 56, 73, 83, 93, 96, 97, 1...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c8f8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    proteins, peptides, lengths, scores = zip(*batch)\n",
    "    \n",
    "    proteins = torch.stack(proteins)        # [B, 34, 1152]\n",
    "    peptides = torch.stack(peptides)        # [B, 21, 1152] — уже паддинг\n",
    "    lengths = torch.tensor(lengths)         # [B]\n",
    "    scores = torch.tensor(scores).unsqueeze(1)  # [B, 1]\n",
    "    \n",
    "    return proteins, peptides, lengths, scores\n",
    "\n",
    "\n",
    "class MHCSequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, score, _, _, _, _, _, alpha_path, beta_path, _, peptide_path, alpha_positions, beta_positions = self.df.loc[idx]\n",
    "        \n",
    "        alpha_embeddings = np.load(alpha_path)[:, ast.literal_eval(alpha_positions), :].squeeze(0)\n",
    "        beta_embeddings = np.load(beta_path)[:, ast.literal_eval(beta_positions), :].squeeze(0)\n",
    "        peptide_embeddings = torch.FloatTensor(np.load(peptide_path)).squeeze(0)\n",
    "\n",
    "        peptide_len = peptide_embeddings.shape[0]\n",
    "\n",
    "        # Паддинг по центру до 21\n",
    "        total_pad = 21 - peptide_len\n",
    "        left_pad = total_pad // 2\n",
    "        right_pad = total_pad - left_pad\n",
    "        peptide_padded = F.pad(peptide_embeddings, (0, 0, left_pad, right_pad), 'constant', value=0)\n",
    "        protein = torch.FloatTensor(np.concatenate([alpha_embeddings, beta_embeddings], axis=0))\n",
    "\n",
    "        return protein, peptide_padded, peptide_len, torch.tensor(score, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0a278324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MHCSequenceDataset(train_data)\n",
    "val_dataset = MHCSequenceDataset(val_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=8, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "#len(train_dataset), train_dataset[30][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "688e9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95533b7",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e653b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(torch.log(torch.tensor(10000.0)) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class CrossAttentionIC50Model(nn.Module):\n",
    "    def __init__(self, d_model=1152, nhead=8, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.protein_pos = PositionalEncoding(d_model, max_len=34)\n",
    "        self.peptide_pos = PositionalEncoding(d_model, max_len=21)\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, 1),\n",
    "            nn.Sigmoid(),  # т.к. IC50 нормализован от 0 до 1\n",
    "        )\n",
    "\n",
    "    def forward(self, protein, peptide):\n",
    "        \"\"\"\n",
    "        protein: [B, 34, 1152]\n",
    "        peptide: [B, L, 1152] (L ∈ [9, 21])\n",
    "        \"\"\"\n",
    "\n",
    "        B, L, D = peptide.size()\n",
    "\n",
    "        # позиционная кодировка\n",
    "        protein = self.protein_pos(protein)\n",
    "        peptide_fwd = self.peptide_pos(peptide)\n",
    "        peptide_rev = self.peptide_pos(torch.flip(peptide, dims=[1]))\n",
    "\n",
    "        # cross-attention (protein queries, peptide keys/values)\n",
    "        attn_out_fwd, _ = self.cross_attn(query=protein, key=peptide_fwd, value=peptide_fwd)\n",
    "        attn_out_rev, _ = self.cross_attn(query=protein, key=peptide_rev, value=peptide_rev)\n",
    "\n",
    "        # Инвариантность ориентации — усреднение\n",
    "        attn_out = (attn_out_fwd + attn_out_rev) / 2  # [B, 34, 1152]\n",
    "\n",
    "        # Пулинг по белку (например, mean pooling)\n",
    "        pooled = attn_out.mean(dim=1)  # [B, 1152]\n",
    "\n",
    "        return self.mlp(pooled)  # [B, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a6367c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, dim, 2) * -(torch.log(torch.tensor(10000.0)) / dim))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class CrossAttentionPairwiseModel(nn.Module):\n",
    "    def __init__(self, d_model=1152, nhead=8, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.protein_pos = PositionalEncoding(d_model, max_len=34)\n",
    "        self.peptide_pos = PositionalEncoding(d_model, max_len=21)\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)\n",
    "\n",
    "        self.pairwise_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def compute_pairwise_scores(self, protein, peptide, pep_lens):\n",
    "        B, N, D = protein.size()  # N = 34\n",
    "        _, M, _ = peptide.size()  # M = 21 (padded)\n",
    "\n",
    "        # Расширяем для батчевого pairwise сравнения\n",
    "        p = protein.unsqueeze(2).expand(B, N, M, D)    # [B, 34, 21, D]\n",
    "        q = peptide.unsqueeze(1).expand(B, N, M, D)    # [B, 34, 21, D]\n",
    "\n",
    "        pairwise = torch.cat([p, q], dim=-1)  # [B, 34, 21, 2D]\n",
    "        scores = self.pairwise_mlp(pairwise).squeeze(-1)  # [B, 34, 21]\n",
    "\n",
    "        # маска паддингов\n",
    "        mask = torch.arange(M, device=peptide.device)[None, :] < pep_lens[:, None]  # [B, 21]\n",
    "        mask = mask.unsqueeze(1).expand(-1, N, -1)  # [B, 34, 21]\n",
    "        scores[~mask] = float('-inf')  # обнуляем паддинги\n",
    "\n",
    "        pooled = torch.max(scores, dim=-1).values  # [B, 34]\n",
    "        pooled = pooled.mean(dim=-1, keepdim=True)  # [B, 1]\n",
    "        return pooled  # scalar signal per sample\n",
    "\n",
    "    def forward(self, protein, peptide, pep_lens):\n",
    "        # Positional encoding\n",
    "        protein = self.protein_pos(protein)\n",
    "        peptide = self.peptide_pos(peptide)\n",
    "\n",
    "        # Cross-attention: protein attends to peptide\n",
    "        attn_out, _ = self.cross_attn(protein, peptide, peptide, key_padding_mask=(~self.build_mask(peptide, pep_lens)))\n",
    "\n",
    "        # Cross-attention pooled\n",
    "        prot_attn_repr = attn_out.mean(dim=1)  # [B, D]\n",
    "\n",
    "        # Pairwise interaction pooled score\n",
    "        pairwise_scalar = self.compute_pairwise_scores(protein, peptide, pep_lens)  # [B, 1]\n",
    "\n",
    "        # Final fusion and regression\n",
    "        combined = torch.cat([prot_attn_repr, pairwise_scalar], dim=-1)  # [B, D + 1]\n",
    "        return self.output_mlp(combined)  # [B, 1]\n",
    "\n",
    "    def build_mask(self, peptide, pep_lens):\n",
    "        B, L, _ = peptide.size()\n",
    "        mask = torch.arange(L, device=peptide.device)[None, :] < pep_lens[:, None]  # [B, L]\n",
    "        return mask == 0  # для key_padding_mask → True = игнорируем\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf3841",
   "metadata": {},
   "source": [
    "# Create lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c74c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_metrics_regression = self._make_metrics_regression(\"train_\")\n",
    "        self.validation_metrics_regression = self._make_metrics_regression(\"validation_\")\n",
    "        self.train_metrics_classification = self._make_metrics_classification(\"train_\")\n",
    "        self.validation_metrics_classification = self._make_metrics_classification(\"validation_\")\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.cutoff = 1.0 - np.log(500) / np.log(50000)\n",
    "\n",
    "    def _make_metrics_classification(self, prefix):\n",
    "        metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "               \"auroc\": AUROC(num_classes=2, task=\"binary\")\n",
    "            },\n",
    "            prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "    def _make_metrics_regression(self, prefix):\n",
    "        metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "                \"pcc\": PearsonCorrCoef(),\n",
    "                \"srcc\": SpearmanCorrCoef(),  \n",
    "                \"r2\": R2Score(),             \n",
    "            },\n",
    "            prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "    # def forward(self, mhc_embeddings, peptide_embeddings, length):\n",
    "    #     return self.model(mhc_embeddings, peptide_embeddings, length)\n",
    "\n",
    "    def forward(self, mhc_embeddings, peptide_embeddings):\n",
    "        return self.model(mhc_embeddings, peptide_embeddings)\n",
    "\n",
    "\n",
    "    def _evaluate(self, batch, stage=None):\n",
    "        mhc_embeddings, peptide_embeddings, length, scores = batch\n",
    "        scores = scores.squeeze()\n",
    "        logits = self.forward(mhc_embeddings, peptide_embeddings).squeeze()\n",
    "        binary_logits = logits >= self.cutoff\n",
    "        loss = self.loss_fn(logits, scores) \n",
    "\n",
    "        \n",
    "\n",
    "        metrics_dict = {f\"{stage}_loss\": loss}\n",
    "\n",
    "        if stage == 'validation':\n",
    "            val_metrics_regression = self.validation_metrics_regression(logits, scores)\n",
    "            val_metrics_classification = self.validation_metrics_classification(binary_logits, scores)\n",
    "            metrics_dict.update(val_metrics_regression)\n",
    "            metrics_dict.update(val_metrics_classification)\n",
    "        elif stage == 'train':\n",
    "            train_metrics_regression = self.train_metrics_regression(logits, scores)\n",
    "            train_metrics_classification = self.train_metrics_classification(binary_logits, scores)\n",
    "            metrics_dict.update(train_metrics_regression)\n",
    "            metrics_dict.update(train_metrics_classification)\n",
    "\n",
    "            self.log_dict(metrics_dict, \n",
    "                          on_step=True, \n",
    "                          on_epoch=False, \n",
    "                          sync_dist=True, \n",
    "                          prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._evaluate(batch, stage='train')\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_metrics_classification.reset()\n",
    "        self.train_metrics_regression.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._evaluate(batch, stage='validation')        \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Логируем валидационные метрики\n",
    "        self.log_dict(self.validation_metrics_regression.compute(), \n",
    "                      on_step=False, \n",
    "                      on_epoch=True, \n",
    "                      sync_dist=True, \n",
    "                      prog_bar=True)\n",
    "        self.validation_metrics_regression.reset()\n",
    "\n",
    "        self.log_dict(self.validation_metrics_classification.compute(), \n",
    "                      on_step=False, \n",
    "                      on_epoch=True, \n",
    "                      sync_dist=True, \n",
    "                      prog_bar=True)\n",
    "        self.validation_metrics_classification.reset()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=self.learning_rate, \n",
    "                                      weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"validation_auroc\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b1275544",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = '/home/user11/results/logs/'\n",
    "log_csv_path = '/home/user11/results/logs_csv/'\n",
    "checkpoints_path = '/home/user11/results/models/'\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7204138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user11/.conda/envs/esm/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model_name = 'CrossAttentionIC50Model'\n",
    "#model_name = 'CrossAttentionPairwiseModel'\n",
    "\n",
    "model = LModel(CrossAttentionIC50Model(),\n",
    "               learning_rate=3e-4,\n",
    "               weight_decay=1e-3,\n",
    "               )\n",
    "\n",
    "logger = pl_loggers.TensorBoardLogger(name=f\"{model_name}\", save_dir=log_path)\n",
    "logger_csv = pl_loggers.CSVLogger(name=f\"{model_name}\", save_dir=log_csv_path)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(checkpoints_path, f\"{model_name}\", f\"version_{logger_csv.version}\", \"checkpoints\"),\n",
    "    filename=\"model-epoch={epoch:02d}\",\n",
    "    save_top_k=-1,\n",
    "    every_n_epochs=1,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "best_iou_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(checkpoints_path, f\"{model_name}\", f\"version_{logger_csv.version}\", \"checkpoints\"),\n",
    "    filename=\"best_model_epoch={epoch:02d}-auroc={validation_auroc:.4f}\",\n",
    "    monitor=\"validation_auroc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"validation_auroc\", patience=10, mode=\"max\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    devices=[0],\n",
    "    default_root_dir=f'{checkpoints_path}/{model_name}',\n",
    "    logger=[logger, logger_csv],\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    callbacks=[best_iou_callback, early_stop, TQDMProgressBar(refresh_rate=1)],\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fd617cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                              | Type                    | Params | Mode \n",
      "--------------------------------------------------------------------------------------\n",
      "0 | model                             | CrossAttentionIC50Model | 7.7 M  | train\n",
      "1 | train_metrics_regression          | MetricCollection        | 0      | train\n",
      "2 | validation_metrics_regression     | MetricCollection        | 0      | train\n",
      "3 | train_metrics_classification      | MetricCollection        | 0      | train\n",
      "4 | validation_metrics_classification | MetricCollection        | 0      | train\n",
      "5 | loss_fn                           | MSELoss                 | 0      | train\n",
      "--------------------------------------------------------------------------------------\n",
      "7.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.706    Total estimated model params size (MB)\n",
      "23        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0134ca189a454225ac14890992b5a75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db70a8f6101b4f448c971664227eb2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                              | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user11/.conda/envs/esm/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c789b56d9c45f59bde433fafb710ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19218f9447b4683b4c9b6e2721d9843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fef8542d3ff47fabd3fa4227515c4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd126dc80b94602aacb3ab90dbfe91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad1d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_kernel",
   "language": "python",
   "name": "esm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
