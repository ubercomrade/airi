{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lightning as L\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import (\n",
    "    AUROC,\n",
    ")\n",
    "from torchmetrics import (\n",
    "    PearsonCorrCoef,\n",
    "    SpearmanCorrCoef,\n",
    "    R2Score\n",
    ")\n",
    "\n",
    "import ast\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748da9f",
   "metadata": {},
   "source": [
    "# Check data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67c6031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/user11/data/data_processed/data.tsv\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "embeddings_table = pd.read_csv(\"/home/user11/data/embeddings_proteins/wide_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "i = 1\n",
    "\n",
    "train = pd.read_csv(f\"/home/user11/data/data_processed/train{i}\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "train.hla = train.hla.str.replace(\"_\", \"\")\n",
    "train_data = pd.merge(train, embeddings_table, on=[\"peptide\", \"score\", \"hla\"])\n",
    "\n",
    "val = pd.read_csv(f\"/home/user11/data/data_processed/test{i}\", sep=\"\\t\", names=[\"peptide\", \"score\", \"hla\"])\n",
    "val.hla = val.hla.str.replace(\"_\", \"\")\n",
    "val_data = pd.merge(val, embeddings_table, on=[\"peptide\", \"score\", \"hla\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28ec3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peptide                                                DLDKKETVWHLEE\n",
       "score                                                            0.0\n",
       "hla                                            HLA-DPA10103-DPB10201\n",
       "alpha_id                                                    DPA10103\n",
       "beta_id                                                     DPB10201\n",
       "alpha_seq          MRPEDRMFHIRAVILRALSLAFLLSLRGAGAIKADHVSTYAAFVQT...\n",
       "beta_seq           MMVLQVSAAPRTVALTALLMVLLTSVVQGRATPENYLFQGRQECYA...\n",
       "alpha_path         /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "beta_path          /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "interface                         YAFFMFSGGAILNTLFGQFEYFDIEEVRMHLGMT\n",
       "peptide_path       /home/user11/data/embeddings_proteins/emb_esmc...\n",
       "alpha_positions    [39, 41, 52, 54, 61, 82, 83, 88, 89, 91, 95, 9...\n",
       "beta_positions     [37, 39, 38, 52, 54, 56, 73, 83, 93, 96, 97, 1...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b6c44",
   "metadata": {},
   "source": [
    "# Базовый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5c8f8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    proteins, peptides, lengths, scores = zip(*batch)\n",
    "    \n",
    "    proteins = torch.stack(proteins)        # [B, 34, 1152]\n",
    "    peptides = torch.stack(peptides)        # [B, 21, 1152] — уже паддинг\n",
    "    lengths = torch.tensor(lengths)         # [B]\n",
    "    scores = torch.tensor(scores).unsqueeze(1)  # [B, 1]\n",
    "    \n",
    "    return proteins, peptides, lengths, scores\n",
    "\n",
    "\n",
    "class MHCSequenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _, score, _, _, _, _, _, alpha_path, beta_path, _, peptide_path, alpha_positions, beta_positions = self.df.loc[idx]\n",
    "        \n",
    "        alpha_embeddings = np.load(alpha_path)[:, ast.literal_eval(alpha_positions), :].squeeze(0)\n",
    "        beta_embeddings = np.load(beta_path)[:, ast.literal_eval(beta_positions), :].squeeze(0)\n",
    "        peptide_embeddings = torch.FloatTensor(np.load(peptide_path)).squeeze(0)\n",
    "\n",
    "        peptide_len = peptide_embeddings.shape[0]\n",
    "\n",
    "        # Паддинг по центру до 21\n",
    "        total_pad = 21 - peptide_len\n",
    "        left_pad = total_pad // 2\n",
    "        right_pad = total_pad - left_pad\n",
    "        peptide_padded = F.pad(peptide_embeddings, (0, 0, left_pad, right_pad), 'constant', value=0)\n",
    "        protein = torch.FloatTensor(np.concatenate([alpha_embeddings, beta_embeddings], axis=0))\n",
    "\n",
    "        return protein, peptide_padded, peptide_len, torch.tensor(score, dtype=torch.float)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b49c8",
   "metadata": {},
   "source": [
    "# Модификация датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9424b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    proteins, peptides, lengths, scores = zip(*batch)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    proteins = torch.stack(proteins)        # [B, 34, 1152]\n",
    "    peptides = torch.stack(peptides)        # [B, 21, 1152]\n",
    "    lengths = torch.tensor(lengths)         # [B]\n",
    "    scores = torch.tensor(scores, dtype=torch.float32).unsqueeze(1)  # [B, 1]\n",
    "    \n",
    "    # Create mask for peptides (1 for real, 0 for padding)\n",
    "    max_len = peptides.size(1)\n",
    "    mask = torch.arange(max_len).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n",
    "    mask = mask.float().unsqueeze(-1)  # [B, 21, 1]\n",
    "    \n",
    "    return proteins, peptides, lengths, scores, mask\n",
    "\n",
    "class MHCSequenceDataset(Dataset):\n",
    "    def __init__(self, df, max_peptide_len=21):\n",
    "        self.df = df\n",
    "        self.max_len = max_peptide_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load embeddings\n",
    "        alpha_emb = np.load(row['alpha_path'])[:, ast.literal_eval(row['alpha_positions']), :].squeeze(0)\n",
    "        beta_emb = np.load(row['beta_path'])[:, ast.literal_eval(row['beta_positions']), :].squeeze(0)\n",
    "        peptide_emb = torch.FloatTensor(np.load(row['peptide_path'])).squeeze(0)\n",
    "        \n",
    "        # Original peptide length (before padding)\n",
    "        peptide_len = peptide_emb.shape[0]\n",
    "        \n",
    "        # Center padding\n",
    "        total_pad = self.max_len - peptide_len\n",
    "        left_pad = total_pad // 2\n",
    "        right_pad = total_pad - left_pad\n",
    "        peptide_padded = F.pad(peptide_emb, (0, 0, left_pad, right_pad), 'constant', 0)\n",
    "        \n",
    "        # Combine protein chains\n",
    "        protein = torch.FloatTensor(np.concatenate([alpha_emb, beta_emb], axis=0))\n",
    "        \n",
    "        return protein, peptide_padded, peptide_len, torch.tensor(row['score'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22427504",
   "metadata": {},
   "source": [
    "# Задаем датасет и лоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0a278324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MHCSequenceDataset(train_data)\n",
    "val_dataset = MHCSequenceDataset(val_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=8, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "#len(train_dataset), train_dataset[30][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "688e9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3bc56125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdfec7",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, LayerNorm\n",
    "\n",
    "\n",
    "# ---------- Dataset Class ----------\n",
    "class ProteinPeptideGNNDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def create_chain_edges(self, length, offset=0):\n",
    "        edges = []\n",
    "        for i in range(length - 1):\n",
    "            edges.append((i + offset, i + 1 + offset))\n",
    "            edges.append((i + 1 + offset, i + offset))\n",
    "        return edges\n",
    "\n",
    "    def create_full_bipartite_edges(self, protein_len, peptide_len):\n",
    "        edges = []\n",
    "        for i in range(protein_len):\n",
    "            for j in range(peptide_len):\n",
    "                edges.append((i, j + protein_len))\n",
    "                edges.append((j + protein_len, i))\n",
    "        return edges\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        _, score, _, _, _, _, _, alpha_path, beta_path, _, peptide_path, alpha_positions, beta_positions = row\n",
    "\n",
    "        alpha_embeddings = np.load(alpha_path)[:, ast.literal_eval(alpha_positions), :].squeeze(0)\n",
    "        beta_embeddings = np.load(beta_path)[:, ast.literal_eval(beta_positions), :].squeeze(0)\n",
    "        protein_embeddings = np.concatenate([alpha_embeddings, beta_embeddings], axis=0)\n",
    "        peptide_embeddings = np.load(peptide_path).squeeze(0)\n",
    "\n",
    "        protein_embeddings = torch.FloatTensor(protein_embeddings)\n",
    "        peptide_embeddings = torch.FloatTensor(peptide_embeddings)\n",
    "\n",
    "        x = torch.cat([protein_embeddings, peptide_embeddings], dim=0)\n",
    "        protein_len = protein_embeddings.size(0)\n",
    "        peptide_len = peptide_embeddings.size(0)\n",
    "\n",
    "        seq_edges = self.create_chain_edges(protein_len) + self.create_chain_edges(peptide_len, offset=protein_len)\n",
    "        int_edges = self.create_full_bipartite_edges(protein_len, peptide_len)\n",
    "        edge_index = torch.tensor(seq_edges + int_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        peptide_mask = torch.zeros(x.size(0), dtype=torch.bool)\n",
    "        peptide_mask[protein_len:] = True\n",
    "        y = torch.tensor([score], dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data.peptide_mask = peptide_mask\n",
    "        data.protein_len = protein_len\n",
    "        data.peptide_len = peptide_len\n",
    "        return data\n",
    "\n",
    "\n",
    "# ---------- GNN Model ----------\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim=1152, hidden_dim=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=2, concat=False)\n",
    "        self.norm1 = LayerNorm(hidden_dim)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim, heads=2, concat=False)\n",
    "        self.norm2 = LayerNorm(hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        batch = data.batch\n",
    "        peptide_mask = data.peptide_mask\n",
    "\n",
    "        x = F.elu(self.norm1(self.conv1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.norm2(self.conv2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        prot_repr = global_mean_pool(x[~peptide_mask], batch[~peptide_mask])\n",
    "        pep_repr = global_mean_pool(x[peptide_mask], batch[peptide_mask])\n",
    "\n",
    "        combined = torch.cat([prot_repr, pep_repr], dim=1)\n",
    "        return self.out(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "090cc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinPeptideGNNDataset(train_data)\n",
    "val_dataset = ProteinPeptideGNNDataset(val_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=8)\n",
    "\n",
    "#len(train_dataset), train_dataset[30][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95533b7",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae560aa5",
   "metadata": {},
   "source": [
    "### 1 (Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e653b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(torch.log(torch.tensor(10000.0)) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class CrossAttentionIC50Model(nn.Module):\n",
    "    def __init__(self, d_model=1152, nhead=8, dim_feedforward=2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.protein_pos = PositionalEncoding(d_model, max_len=34)\n",
    "        self.peptide_pos = PositionalEncoding(d_model, max_len=21)\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, 1),\n",
    "            #nn.Sigmoid(),  # т.к. IC50 нормализован от 0 до 1\n",
    "        )\n",
    "\n",
    "    def forward(self, protein, peptide):\n",
    "        \"\"\"\n",
    "        protein: [B, 34, 1152]\n",
    "        peptide: [B, L, 1152] (L ∈ [9, 21])\n",
    "        \"\"\"\n",
    "\n",
    "        B, L, D = peptide.size()\n",
    "\n",
    "        # позиционная кодировка\n",
    "        protein = self.protein_pos(protein)\n",
    "        peptide_fwd = self.peptide_pos(peptide)\n",
    "        peptide_rev = self.peptide_pos(torch.flip(peptide, dims=[1]))\n",
    "\n",
    "        # cross-attention (protein queries, peptide keys/values)\n",
    "        attn_out_fwd, _ = self.cross_attn(query=protein, key=peptide_fwd, value=peptide_fwd)\n",
    "        attn_out_rev, _ = self.cross_attn(query=protein, key=peptide_rev, value=peptide_rev)\n",
    "\n",
    "        # Инвариантность ориентации — усреднение\n",
    "        attn_out = (attn_out_fwd + attn_out_rev) / 2  # [B, 34, 1152]\n",
    "\n",
    "        # Пулинг по белку (например, mean pooling)\n",
    "        pooled = attn_out.mean(dim=1)  # [B, 1152]\n",
    "\n",
    "        return self.mlp(pooled)  # [B, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daba677",
   "metadata": {},
   "source": [
    "#### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, dim, max_len=1000):\n",
    "#         super().__init__()\n",
    "#         position = torch.arange(0, max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, dim, 2) * -(torch.log(torch.tensor(10000.0)) / dim))\n",
    "#         pe = torch.zeros(max_len, dim)\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, dim]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# class CrossAttentionIC50Model(nn.Module):\n",
    "#     def __init__(self, d_model=1152, nhead=8, dim_feedforward=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.protein_pos = PositionalEncoding(d_model, max_len=34)\n",
    "#         self.peptide_pos = PositionalEncoding(d_model, max_len=21)\n",
    "\n",
    "#         self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "#         self.feedforward = nn.Sequential(\n",
    "#             nn.Linear(d_model, dim_feedforward),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(dim_feedforward, d_model),\n",
    "#         )\n",
    "\n",
    "#         self.out = nn.Sequential(\n",
    "#             nn.Linear(d_model, dim_feedforward // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(dim_feedforward // 2, 1),\n",
    "#             nn.Sigmoid(),  # Output is normalized IC50 ∈ [0, 1]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, protein, peptide):\n",
    "#         \"\"\"\n",
    "#         protein: [B, 34, 1152]\n",
    "#         peptide: [B, L, 1152] (L ∈ [9, 21])\n",
    "#         \"\"\"\n",
    "#         B, L, D = peptide.size()\n",
    "\n",
    "#         # Positional encoding\n",
    "#         protein = self.protein_pos(protein)\n",
    "#         peptide_fwd = self.peptide_pos(peptide)\n",
    "#         peptide_rev = self.peptide_pos(torch.flip(peptide, dims=[1]))\n",
    "\n",
    "#         # Cross-attention (protein queries, peptide keys/values)\n",
    "#         attn_out_fwd, _ = self.cross_attn(query=protein, key=peptide_fwd, value=peptide_fwd)\n",
    "#         attn_out_rev, _ = self.cross_attn(query=protein, key=peptide_rev, value=peptide_rev)\n",
    "\n",
    "#         # Orientation invariance: average\n",
    "#         attn_out = (attn_out_fwd + attn_out_rev) / 2  # [B, 34, D]\n",
    "\n",
    "#         # Add & Norm\n",
    "#         x = self.norm1(protein + attn_out)\n",
    "\n",
    "#         # Feedforward block (like Transformer block)\n",
    "#         ff_out = self.feedforward(x)\n",
    "#         x = self.norm2(x + ff_out)\n",
    "\n",
    "#         # Mean pooling over protein sequence\n",
    "#         pooled = x.mean(dim=1)  # [B, D]\n",
    "\n",
    "#         return self.out(pooled)  # [B, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88767190",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c444a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProteinPeptideInteractionModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=1152, hidden_dim=512, num_heads=4):\n",
    "        super(ProteinPeptideInteractionModel, self).__init__()\n",
    "        \n",
    "        self.peptide_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.protein_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            #nn.Sigmoid()  # так как выход от 0 до 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, protein, peptide):\n",
    "        # protein: [B, 34, 1152]\n",
    "        # peptide: [B, 21, 1152]\n",
    "        \n",
    "        # Проецируем эмбеддинги\n",
    "        protein_proj = self.protein_proj(protein)   # [B, 34, hidden]\n",
    "        peptide_proj = self.peptide_proj(peptide)   # [B, 21, hidden]\n",
    "        \n",
    "        # Кросс-аттеншн: пептид (query) взаимодействует с белком (key, value)\n",
    "        attn_output, _ = self.cross_attn(query=peptide_proj,\n",
    "                                         key=protein_proj,\n",
    "                                         value=protein_proj)\n",
    "        # Агрегируем: берем среднее по всем позициям пептида\n",
    "        attn_repr = attn_output.mean(dim=1)        # [B, hidden]\n",
    "        pep_repr = peptide_proj.mean(dim=1)        # [B, hidden]\n",
    "        \n",
    "        combined = torch.cat([attn_repr, pep_repr], dim=1)  # [B, hidden*2]\n",
    "        \n",
    "        output = self.fc(combined)  # [B, 1], от 0 до 1\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f717b5f",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0ed1ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model=1152, nhead=8, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.protein_pos = PositionalEncoding(d_model, max_len=34)\n",
    "        self.peptide_pos = PositionalEncoding(d_model, max_len=21)\n",
    "        \n",
    "        # Multi-head attention с layer norm\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # FFN с residual connection\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        \n",
    "        # Final MLP с улучшениями\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim_feedforward, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, protein, peptide):\n",
    "        # Позиционные кодировки\n",
    "        protein = self.protein_pos(protein)\n",
    "        peptide = self.peptide_pos(peptide)\n",
    "        \n",
    "        # Cross-attention с residual\n",
    "        attn_out, _ = self.attn(\n",
    "            query=protein,\n",
    "            key=peptide,\n",
    "            value=peptide,\n",
    "            need_weights=False\n",
    "        )\n",
    "        attn_out = self.norm1(protein + attn_out)\n",
    "        \n",
    "        # FFN с residual\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        ffn_out = self.norm2(attn_out + ffn_out)\n",
    "        \n",
    "        # Усреднение с инвариантностью к ориентации\n",
    "        rev_out, _ = self.attn(\n",
    "            query=protein,\n",
    "            key=torch.flip(peptide, [1]),\n",
    "            value=torch.flip(peptide, [1]),\n",
    "            need_weights=False\n",
    "        )\n",
    "        combined = (ffn_out + rev_out) / 2\n",
    "        \n",
    "        # Улучшенный пулинг\n",
    "        pooled = combined.max(dim=1)[0]  # Max pooling вместо mean\n",
    "        return self.mlp(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fbe95",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a6367c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PairwiseEnergyModel(nn.Module):\n",
    "    def __init__(self, embed_dim=1152, hidden_dim=512, dropout=0.3):\n",
    "        super(PairwiseEnergyModel, self).__init__()\n",
    "        # MLP для вычисления энергии E(i,j) для пар (protein[i], peptide[j])\n",
    "        self.energy_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        # Финальный слой для предсказания IC50\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(34 * 21, hidden_dim),  # 34 (protein) x 21 (peptide)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, proteins, peptides, lengths):\n",
    "        batch_size = proteins.size(0)\n",
    "        protein_len, peptide_max_len = 34, 21  # Фиксированные размеры из датасета\n",
    "\n",
    "        # Подготовка для pairwise-взаимодействий\n",
    "        proteins = proteins.unsqueeze(2).expand(-1, -1, peptide_max_len, -1)  # [B, 34, 21, 1152]\n",
    "        peptides = peptides.unsqueeze(1).expand(-1, protein_len, -1, -1)     # [B, 34, 21, 1152]\n",
    "        \n",
    "        # Конкатенация парного представления\n",
    "        pairwise_input = torch.cat([proteins, peptides], dim=-1)  # [B, 34, 21, 1152*2]\n",
    "        \n",
    "        # Вычисление энергий E(i,j)\n",
    "        energies = self.energy_mlp(pairwise_input).squeeze(-1)  # [B, 34, 21]\n",
    "        \n",
    "        # Softmax для нормировки энергий\n",
    "        energies = F.softmax(energies.view(batch_size, -1), dim=-1)  # [B, 34*21]\n",
    "        \n",
    "        # Финальное предсказание\n",
    "        score = self.final_mlp(energies)  # [B, 1]\n",
    "        return score\n",
    "\n",
    "# # Пример использования\n",
    "# model = PairwiseEnergyModel(embed_dim=1152, hidden_dim=512, dropout=0.3)\n",
    "# proteins, peptides, lengths, scores = next(iter(DataLoader(dataset, batch_size=32, collate_fn=collate_fn)))\n",
    "# output = model(proteins, peptides, lengths)  # [B, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c6e95",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ea4f8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DualEncoderModel(nn.Module):\n",
    "    def __init__(self, embed_dim=1152, num_heads=8, num_layers=2, hidden_dim=512, dropout=0.3):\n",
    "        super(DualEncoderModel, self).__init__()\n",
    "        # Трансформер-энкодер для белка\n",
    "        self.protein_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Трансформер-энкодер для пептида\n",
    "        self.peptide_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Interaction Head\n",
    "        self.interaction_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, proteins, peptides, lengths):\n",
    "        batch_size = proteins.size(0)\n",
    "        \n",
    "        # Трансформер для белка: [B, 34, 1152] -> [B, 34, 1152]\n",
    "        protein_out = self.protein_encoder(proteins)  # [B, 34, 1152]\n",
    "        protein_out = protein_out.mean(dim=1)  # Pooling: [B, 1152]\n",
    "        \n",
    "        # Трансформер для пептида: [B, 21, 1152] -> [B, 21, 1152]\n",
    "        peptide_out = self.peptide_encoder(peptides)  # [B, 21, 1152]\n",
    "        peptide_out = peptide_out.mean(dim=1)  # Pooling: [B, 1152]\n",
    "        \n",
    "        # Объединение представлений\n",
    "        interaction = torch.cat([protein_out, peptide_out], dim=-1)  # [B, 1152*2]\n",
    "        \n",
    "        # Предсказание IC50\n",
    "        score = self.interaction_head(interaction)  # [B, 1]\n",
    "        return score\n",
    "\n",
    "# # Пример использования\n",
    "# model = DualEncoderModel(embed_dim=1152, num_heads=8, num_layers=2, hidden_dim=512, dropout=0.3)\n",
    "# proteins, peptides, lengths, scores = next(iter(DataLoader(dataset, batch_size=32, collate_fn=collate_fn)))\n",
    "# output = model(proteins, peptides, lengths)  # [B, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17dad40",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "63c45cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNInteractionModel(nn.Module):\n",
    "    def __init__(self, embed_dim=1152, num_filters=128, kernel_size=3, hidden_dim=512, dropout=0.3):\n",
    "        super(CNNInteractionModel, self).__init__()\n",
    "        \n",
    "        # CNN-энкодер для белка\n",
    "        self.protein_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.AdaptiveMaxPool1d(1)  # Global Max Pooling\n",
    "        )\n",
    "        \n",
    "        # CNN-энкодер для пептида\n",
    "        self.peptide_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.AdaptiveMaxPool1d(1)  # Global Max Pooling\n",
    "        )\n",
    "        \n",
    "        # Interaction Head\n",
    "        self.interaction_head = nn.Sequential(\n",
    "            nn.Linear(num_filters * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, proteins, peptides, lengths):\n",
    "        batch_size = proteins.size(0)\n",
    "        \n",
    "        # Подготовка данных: [B, 34, 1152] -> [B, 1152, 34] для CNN\n",
    "        proteins = proteins.permute(0, 2, 1)  # [B, 1152, 34]\n",
    "        peptides = peptides.permute(0, 2, 1)  # [B, 1152, 21]\n",
    "        \n",
    "        # CNN для белка\n",
    "        protein_out = self.protein_cnn(proteins).squeeze(-1)  # [B, num_filters]\n",
    "        \n",
    "        # CNN для пептида\n",
    "        peptide_out = self.peptide_cnn(peptides).squeeze(-1)  # [B, num_filters]\n",
    "        \n",
    "        # Объединение представлений\n",
    "        interaction = torch.cat([protein_out, peptide_out], dim=-1)  # [B, num_filters*2]\n",
    "        \n",
    "        # Предсказание IC50\n",
    "        score = self.interaction_head(interaction)  # [B, 1]\n",
    "        return score\n",
    "\n",
    "# Пример использования\n",
    "# model = CNNInteractionModel(embed_dim=1152, num_filters=128, kernel_size=3, hidden_dim=512, dropout=0.3)\n",
    "# proteins, peptides, lengths, scores = next(iter(DataLoader(dataset, batch_size=32, collate_fn=collate_fn)))\n",
    "# output = model(proteins, peptides, lengths)  # [B, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada74880",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf3841",
   "metadata": {},
   "source": [
    "# Create lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "9c74c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.model = model\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_metrics_regression = self._make_metrics_regression(\"train_\")\n",
    "        self.validation_metrics_regression = self._make_metrics_regression(\"validation_\")\n",
    "        self.train_metrics_classification = self._make_metrics_classification(\"train_\")\n",
    "        self.validation_metrics_classification = self._make_metrics_classification(\"validation_\")\n",
    "\n",
    "        #self.loss_fn = nn.MSELoss()\n",
    "        #self.loss_fn = nn.HuberLoss()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.cutoff = 1.0 - np.log(500) / np.log(50000)\n",
    "\n",
    "    def _make_metrics_classification(self, prefix):\n",
    "        metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "               \"auroc\": AUROC(num_classes=2, task=\"binary\")\n",
    "            },\n",
    "            prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "    def _make_metrics_regression(self, prefix):\n",
    "        metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "                \"pcc\": PearsonCorrCoef(),\n",
    "                #\"srcc\": SpearmanCorrCoef(),  \n",
    "                \"r2\": R2Score(),             \n",
    "            },\n",
    "            prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    # def forward(self, mhc_embeddings, peptide_embeddings):\n",
    "    #     return self.model(mhc_embeddings, peptide_embeddings)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "\n",
    "\n",
    "    def _evaluate(self, batch, stage=None):\n",
    "\n",
    "        # mhc_embeddings, peptide_embeddings, length, scores = batch\n",
    "        # binary_scores = (scores >= self.cutoff).float()\n",
    "        # logits = self.forward(mhc_embeddings, peptide_embeddings)#.squeeze()        \n",
    "        # probs = logits.sigmoid()\n",
    "        # #loss = self.loss_fn(logits, binary_scores) # For BCE\n",
    "        # loss = self.loss_fn(probs, scores) # For regression\n",
    "\n",
    "        scores = batch.y\n",
    "        binary_scores = (scores >= self.cutoff).float()\n",
    "        logits = self.forward(batch).squeeze()\n",
    "        probs = logits.sigmoid()\n",
    "        #loss = self.loss_fn(probs, scores)\n",
    "        loss = self.loss_fn(logits, binary_scores) # For BCE\n",
    "\n",
    "\n",
    "        metrics_dict = {f\"{stage}_loss\": loss}\n",
    "\n",
    "        if stage == 'validation':\n",
    "            val_metrics_regression = self.validation_metrics_regression(probs, scores)\n",
    "            val_metrics_classification = self.validation_metrics_classification(probs, binary_scores)\n",
    "            metrics_dict.update(val_metrics_regression)\n",
    "            metrics_dict.update(val_metrics_classification)\n",
    "        elif stage == 'train':\n",
    "            train_metrics_regression = self.train_metrics_regression(probs, scores)\n",
    "            train_metrics_classification = self.train_metrics_classification(probs, binary_scores)\n",
    "            metrics_dict.update(train_metrics_regression)\n",
    "            metrics_dict.update(train_metrics_classification)\n",
    "\n",
    "            self.log_dict(metrics_dict, \n",
    "                          on_step=True, \n",
    "                          on_epoch=False, \n",
    "                          sync_dist=True, \n",
    "                          prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._evaluate(batch, stage='train')\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_metrics_classification.reset()\n",
    "        self.train_metrics_regression.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._evaluate(batch, stage='validation')        \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Логируем валидационные метрики\n",
    "        self.log_dict(self.validation_metrics_regression.compute(), \n",
    "                      on_step=False, \n",
    "                      on_epoch=True, \n",
    "                      sync_dist=True, \n",
    "                      prog_bar=True)\n",
    "        self.validation_metrics_regression.reset()\n",
    "\n",
    "        self.log_dict(self.validation_metrics_classification.compute(), \n",
    "                      on_step=False, \n",
    "                      on_epoch=True, \n",
    "                      sync_dist=True, \n",
    "                      prog_bar=True)\n",
    "        self.validation_metrics_classification.reset()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), \n",
    "                                      lr=self.learning_rate, \n",
    "                                      weight_decay=self.weight_decay)\n",
    "        \n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs,  # или конкретное число, например 50\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 20,\n",
    "            }\n",
    "        }\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optimizer,\n",
    "        #     mode='max',\n",
    "        #     factor=0.5,\n",
    "        #     patience=5,\n",
    "        #     min_lr=1e-6\n",
    "        # )\n",
    "        \n",
    "        # return {\n",
    "        #     \"optimizer\": optimizer,\n",
    "        #     \"lr_scheduler\": {\n",
    "        #         \"scheduler\": scheduler,\n",
    "        #         \"monitor\": \"validation_auroc\",\n",
    "        #         \"interval\": \"epoch\",\n",
    "        #         \"frequency\": 1,\n",
    "        #     },\n",
    "        # }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b1275544",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = '/home/user11/results/logs/'\n",
    "log_csv_path = '/home/user11/results/logs_csv/'\n",
    "checkpoints_path = '/home/user11/results/models/'\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "7204138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model_name = 'CrossAttentionPairwiseModel'\n",
    "\n",
    "\n",
    "# model_name = 'PairwiseEnergyModel'\n",
    "# obj_model = PairwiseEnergyModel(embed_dim=1152, hidden_dim=512, dropout=0.3)\n",
    "\n",
    "# model_name = 'DualEncoderModel'\n",
    "# obj_model = DualEncoderModel(embed_dim=1152, num_heads=4, num_layers=2, hidden_dim=256, dropout=0.3)\n",
    "\n",
    "# model_name = 'CNNInteractionModel'\n",
    "# obj_model = CNNInteractionModel(embed_dim=1152, num_filters=128, kernel_size=3, hidden_dim=512, dropout=0.3)\n",
    "\n",
    "# model_name = 'ImprovedCrossAttention'\n",
    "# obj_model = ImprovedCrossAttention(d_model=1152, nhead=8, dim_feedforward=2048)\n",
    "    \n",
    "# model_name = 'CrossAttentionIC50Model'\n",
    "# obj_model = CrossAttentionIC50Model()\n",
    "\n",
    "# model_name = 'CrossAttentionIC50Model'\n",
    "# obj_model = CrossAttentionIC50Model(d_model=1152, nhead=4, dim_feedforward=2048, dropout=0.2)\n",
    "\n",
    "\n",
    "# best\n",
    "# model_name = 'ProteinPeptideInteractionModel'\n",
    "# obj_model = ProteinPeptideInteractionModel(embedding_dim=1152, hidden_dim=512, num_heads=4)\n",
    "\n",
    "model_name = 'GNNModel'\n",
    "obj_model = GNNModel(input_dim=1152, hidden_dim=256, dropout=0.3)\n",
    "\n",
    "\n",
    "model = LModel(obj_model,\n",
    "               learning_rate=1e-4,\n",
    "               weight_decay=1e-3,\n",
    "               )\n",
    "\n",
    "logger = pl_loggers.TensorBoardLogger(name=f\"{model_name}\", save_dir=log_path)\n",
    "logger_csv = pl_loggers.CSVLogger(name=f\"{model_name}\", save_dir=log_csv_path)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(checkpoints_path, f\"{model_name}\", f\"version_{logger_csv.version}\", \"checkpoints\"),\n",
    "    filename=\"model-epoch={epoch:02d}\",\n",
    "    save_top_k=-1,\n",
    "    every_n_epochs=1,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "best_iou_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(checkpoints_path, f\"{model_name}\", f\"version_{logger_csv.version}\", \"checkpoints\"),\n",
    "    filename=\"best_model_epoch={epoch:02d}-auroc={validation_auroc:.4f}\",\n",
    "    monitor=\"validation_auroc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"validation_auroc\", patience=10, mode=\"max\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    devices=[0],\n",
    "    default_root_dir=f'{checkpoints_path}/{model_name}',\n",
    "    logger=[logger, logger_csv],\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    callbacks=[best_iou_callback, early_stop, TQDMProgressBar(refresh_rate=1)],\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd617cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                              | Type              | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model                             | GNNModel          | 790 K  | train\n",
      "1 | train_metrics_regression          | MetricCollection  | 0      | train\n",
      "2 | validation_metrics_regression     | MetricCollection  | 0      | train\n",
      "3 | train_metrics_classification      | MetricCollection  | 0      | train\n",
      "4 | validation_metrics_classification | MetricCollection  | 0      | train\n",
      "5 | loss_fn                           | BCEWithLogitsLoss | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "790 K     Trainable params\n",
      "0         Non-trainable params\n",
      "790 K     Total params\n",
      "3.161     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e22728454354ceabeddbb815781ee12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2633e586f7f849718ffd30a40c587354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                              | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f860f36146a44c1bd36273ea2d46d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d4c22a53ba46d4867865fd671749f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c139e35e8f4679924e7fc25a5e79e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9ddd0db9964d74b8d9158f9954e3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6f65429d2d446fa0e12f270add38ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0964f26c467435fa90a34378cfd149b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74709807f094a2e8040b796d0100fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dc0684acbe4bda9e3d98a115536348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b639ad1291945fd931db0c0a8670d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6bb6387f634c9cb51cf79381eb1578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259f9141dbfc43589e11b347d3c839f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94633699fbce431299f47087e9a0a974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814dd8148fd646b3b667e57339ad96bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aaa6a4fba842dcac3ca1546eebaa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e3f8135212490a8afe6847152f7f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4421fe6200d4699b1b0fbd791f50db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3fc6fb724d4aecafc40552e25b34d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c057a8bea9441c5b92f81d0eaf71960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8884ad8ce43b451f8c9d38cbc84c6cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5292b3baea4fb38e1e8d3378d97894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b16b88f13674579a5e7c01c39535ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889c4d5c01a94a8fb1139619bd6a03fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c39669a48bc42ffaee4b1e453018e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59cf1e0f4764702b38eed9b2a454d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd777a1c577b484c8fe79c7381d66528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e47c3926b14765af676552e0b8792d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28a6062f5414cbda3200bc35bd854ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3478223ad2444b8465168d35036f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06917f682f64fafb225fe1ceb27dcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcc7c28b70945f5b1d3a00fd2368854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d66189721245eb828d6038037953a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7f80d320346bb9053c61bebb38cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6306ff51e281419faa751f6a7cbb0561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c579d43a489b49238a97e581a4cc0085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa5e1a32e914c81847c16f1ddf03bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27646bba3e6a4cd4b38edf4906bb0edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fefa06a88d242a4a11552ff88c350bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea508f184be4c95b7c822a7b2e021b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a7f177cb584113bd5ce4d884db3941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56e82ba05c4fb8918940959ebedecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d266088a4ed4fd7946be1effeb423d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecf3ae79a034f9aa8fadf08d3134889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22235086438040a6bd1a26c2940f27dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f55f10b73a140fc8e7bc47b53c227c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a55a2a42354f6dba5e30a8cd3804ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4554548159d4751afdc08c62852295b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2ea89dd4224256825ce2936170d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c711f683d9b444ba89ac5a9524205ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1bad1d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(0.0150),\n",
       " 'train_pcc': tensor(0.7141),\n",
       " 'train_r2': tensor(0.4999),\n",
       " 'train_auroc': tensor(0.8438),\n",
       " 'validation_pcc': tensor(0.6301),\n",
       " 'validation_r2': tensor(0.3795),\n",
       " 'validation_auroc': tensor(0.8139)}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_kernel",
   "language": "python",
   "name": "esm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
